# Deita

Deita (**D**ata-**E**fficient **I**nstruction **T**uning for **A**lignment) is an open-source project designed to facilitate automatic data selection for instruction tuning in Large Language Models (LLMs).

It concludes:
- Toolkits for Automatic Data Selection
- Deita Datasets
- Deita-Family Models

:bell: Our model Deita-2-10k-13b through **only supervised fine-tuning (sft)** with **only 10k** data based on LLaMA-2-13B achieves **6.75** on [MT-Bench](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) benchmark, which even outperforms [LLaMA-13B-Chat](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) with more fancy alignment techniques!

For more details, please refer to our paper: [What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning]()

## News
...

## Contents

- [News](#news)
- [:magic\_wand: Automatic Data Selection](#magic_wand-automatic-data-selection)
  - [Overview](#overview)
  - [Data Scorer](#data-scorer)
  - [Pipelines](#pipelines)
- [:rocket: Deita-Family](#rocket-deita-family)
  - [Deita Dataset](#deita-dataset)
  - [Deita Models](#deita-models)
- [TODO](#todo)
- [Citations](#citations)


## :magic_wand: Automatic Data Selection

### Overview

### Data Scorer

### Pipelines

## :rocket: Deita-Family

### Deita Dataset

### Deita Models

## TODO

- [ ] Deita Pipelines
- [ ] CLI-Interface Supported

## Citations
...
